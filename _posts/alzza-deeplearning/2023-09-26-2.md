---
title: "회귀 분석 - 전복의 고리수 추정 신경망"
last_modified_at: 2023-09-25T23:53:12+09:00
categories:
    - alzza-deeplearning
tags:
    - A.I

toc: true
toc_label: "My Table of Contents"
author_profile: true

---
# 목적
[파이썬 날코딩으로 알고 짜는 딥러닝] 책에서 첫번째로 나오는 전복의 고리 수 추정 신경망 파이썬 코드를 분석해보고자 한다.
# 데이터
kaggle의 전복 데이터셋은 4000여마리의 전복에 대해 8가지 특징값과 전복의 고리 수가 들어있다.

# 단층 퍼셉트론 신경망
[https://minchocoin.github.io/alzza-deeplearning/1/](https://minchocoin.github.io/alzza-deeplearning/1/)

# 코드 분석

## 파이썬 모듈 불러들이기
```py
import numpy as np
import csv
import time

np.random.seed(1234)
def randomize(): np.random.seed(time.time())
```
행렬 연산 등에 사용하는 넘파이 모듈과 데이터셋 csv파일을 읽는데 사용하는 csv모듈, 난수 초기화에 사용하는 time 모듈이 있다. 랜덤시드는 1234로 고정이며, 원한다면 randomize()로 랜덤화할 수도 있다.

## 하이퍼파라미터값의 정의
```py
RND_MEAN = 0
RND_STD = 0.0030

LEARNING_RATE = 0.001
```
정규분포 난수값에서 RND_MEAN는 평균이고, RND_STD는 표준편차이다. 즉 저 값으로 설정하고 정규분포 난수값을 무수히 많이 생성한다면, 0이 가장 많이 생성되고, 0에서 멀어질수록 덜 생성되는, 숫자별 확률분포를 그린다면 정규분포(0,0.003)가 나올 것이다. LEARNING_RATE는 학습률이다.

## 실험용 메인 함수 정의

```py
def abalone_exec(epoch_count=10, mb_size=10, report=1):
    load_abalone_dataset()
    init_model()
    train_and_test(epoch_count, mb_size, report)
```
위 함수는 먼저 데이터셋을 읽는 함수를 호출하고, 모델의 파라미터를 초기화하는 함수를 호출하고, 마지막으로 매개변수로 받은 epoch_count,mb_size,report를 train_and_test함수에 전달하고 호출하여 실제로 훈련과 테스트를 한다.

## 데이터 적재 함수 정의

```py
def load_abalone_dataset():
    with open('../../data/chap01/abalone.csv') as csvfile:
        csvreader = csv.reader(csvfile)
        next(csvreader, None)
        rows = []
        for row in csvreader:
            rows.append(row)
            
    global data, input_cnt, output_cnt
    input_cnt, output_cnt = 10, 1
    data = np.zeros([len(rows), input_cnt+output_cnt])

    for n, row in enumerate(rows):
        if row[0] == 'I': data[n, 0] = 1
        if row[0] == 'M': data[n, 1] = 1
        if row[0] == 'F': data[n, 2] = 1
        data[n, 3:] = row[1:]
```
먼저 csv파일을 연다. 그리고 next함수를 이용하여 헤더정보는 읽지 않도록 한다. 그리고 csvreader의 각 행을 rows라는 배열에 넣는다.

그리고 input_cnt와 output_cnt에는 각각 10과 1이 들어가는데, 각각 입력 벡터의 크기와 출력 벡터의 크기이다. data는 입출력벡터 정보를 저장하는 공간이다. rows 배열과의 차이점은 data는 성별 정보가 원-핫 벡터로 표시된다는 점이다.

data의 0번째,1번째,2번째 열을 성별로 사용하는데, 유충이면 0번째열만, 수컷이면 1번째열만 , 암컷이면 2번째열만 1로 설정한다. data의 3번째 열부터는 rows의 데이터를 그대로 붙인다.

## 파라미터 초기화 함수 정의
```py
def init_model():
    global weight, bias, input_cnt, output_cnt
    weight = np.random.normal(RND_MEAN, RND_STD,[input_cnt, output_cnt])
    bias = np.zeros([output_cnt])
```
weight 행렬의 크기를 [10,1]로 하고, bias의 크기를 [1]로 하여 각각 초기화한다. weight는 정규분포를 갖는 난숫값으로 초기화하는데, 경사하강법을 시작할 때 다양한 출발지에서 출발하도록 하기 위함이다. 즉 파라미터의 초기값을 여러개 해보는 것이다. bias는 초기에는 오히려 학습에 역효과만 주기때문에 0으로 한다.

## 학습 및 평가 함수 정의

```py
def train_and_test(epoch_count, mb_size, report):
    step_count = arrange_data(mb_size)
    test_x, test_y = get_test_data()
    
    for epoch in range(epoch_count):
        losses, accs = [], []
        
        for n in range(step_count):
            train_x, train_y = get_train_data(mb_size, n)
            loss, acc = run_train(train_x, train_y)
            losses.append(loss)
            accs.append(acc)
            
        if report > 0 and (epoch+1) % report == 0:
            acc = run_test(test_x, test_y)
            print('Epoch {}: loss={:5.3f}, accuracy={:5.3f}/{:5.3f}'. \
                  format(epoch+1, np.mean(losses), np.mean(accs), acc))
            
    final_acc = run_test(test_x, test_y)
    print('\nFinal Test: final accuracy = {:5.3f}'.format(final_acc))
```
먼저 epoch_count 만큼 학습을 반복하게 되어있다. 1번 학습시, mb_size만큼 쪼갠 데이터를 step_count(=학습용 데이터의 양 // mb_size)만큼 돌며 순서대로 학습하여 학습용 데이터를 모두 학습하도록 되어있다.

미니배치크기의 데이터 학습은 먼저 get_train_data로 학습용 데이터를 얻어와 run_train() 함수로 학습시키고, 손실과 정확도를 리턴받아 리스트에 넣는다. 만약 보고주기가 되면 run_test()로 테스트데이터를 이용하여 정확도를 측정하여 보여준다.

그리고 최종적으로 run_test()를 돌리고 최종 정확도를 출력한다.

## 학습 및 평가 데이터 획득 함수 정의


