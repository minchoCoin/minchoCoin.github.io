---
title: "단층 퍼셉트론(SLP) - 이진 판단"
last_modified_at: 2023-10-01T12:53:12+09:00
categories:
    - alzza-deeplearning
tags:
    - A.I

toc: true
toc_label: "My Table of Contents"
author_profile: true

---

# 이진 판단 문제의 신경망 처리
이진 판단 문제는 예/아니오 혹은 0/1 같은 두 가지 값 중 하나로 답하는 문제다. 가중치와 편향을 이용하는 퍼셉트론의 연산은 두 가지 값으로 결과를 제한하기 힘들다.

이 때문에 선형 연산에서 범위에 제한이 없는 실숫값을 생산하고, 이를 확률값의 성질에 맞게 변환해주는 비선형함수인 시그모이드 함수를 사용한다.

이제 시그모이드 함수에 맞는 손실함수를 정의하여야한다. 즉 값이 0 이상이면서 추정이 정확해질수록 작아지는 성질이 있는 손실 함수를 정의해야한다.

시그모이드 함수에서는 손실함수를 교차 엔트로피로 사용한다. 교차 엔트로피란 두 가지 확률 분포가 얼마나 다른지를 숫자 하나로 표현해주는 것으로, 이진 판단 문제를 다룰 수 있게 되었다.

왜 시그모이드 함수에 MSE를 사용하면 안되는지는 시그모이드 함수를 살펴보고 정리하자.

# 시그모이드 함수
시그모이드 함수는 범위에 제한이 없는 임의의 실숫값을 입력으로 받아 확률값의 범위에 해당하는 0과 1 사이의 값을 출력하는 함수이다. $\sigma (x)$ 로 표현한다. 여기서 입력x를 확률값의 logit 표현이라고 한다. 

로짓값은 상대적이다. A 확률의 로짓값이 5, B확률의 로짓값이 2라면 A확률은 B확률보다 $e^{5-2} = e^3$ 배 정도 크다.

시그모이드 함수는 입력값을 답이 참일 가능성을 로짓으로 표시한 값으로 간주한다. 이때 답이 거짓일 경우의 로짓값을 0으로 간주한다.

예를 들어 A일 가능성의 로짓값이 0.5일 경우 A가 아닐 가능성의 로짓값은 0이고 두 확률의 합은 1이어야되므로, $e^0.5$ 와 $e^0$ 의 합 대비 $e^0.5$ 와 $e^0$ 의 비율을 보면된다. 따라서 A일 확률은 $\frac{e^0.5}{e^0.5 + 1}$ 이고, A가 아닐 확률은 $\frac{1}{e^0.5 + 1}$ 이다.

이를 일반화하면 답이 참일 가능성의 로짓값 x, 답이 거짓일 가능성의 로짓값을 0이라 할때 답이 참일 확률은 $\frac{e^x}{e^x + 1}$ 이고 분모와 분자를 $e^x$ 로 나누면 $\frac{1}{1+e^{-x}}$ 이다. 따라서 시그모이드 함수는 다음과 같이 정의된다.

$$ \sigma (x) = \frac{1}{1+e^{-x}}$$

또한 시그모이드 함수의 그래프는 아래와 같다.
![graph of sigmoid function](https://github.com/minchoCoin/minchoCoin.github.io/assets/62372650/b004a023-c67d-48a1-8486-f11186eea1af)

(사진1 : 시그모이드 함수의 그래프: Powered by desmos)

또한 0.51의 확률로 참을 선택하는 것과 0.99의 확률로 참을 선택하는 것은 의미가 다르다. 0.51의 확률로 참을 선택했을 때보다, 0.99의 확률로 참을 선택했는데 거짓일 경우 더 많은 파라미터 수정이 이루어져야한다. 그러나 시그모이드 함수에 MSE를 적용하면 그것이 불가능하다.

![시그모이드 함수의 MSE적용한 그래프](https://github.com/minchoCoin/minchoCoin.github.io/assets/62372650/09600ce2-9830-4ee5-afe9-24e57932bcfa)

(사진2: 시그모이드 함수에 MSE를 적용한 그래프, 실제 확률이 1이라고 가정, 입력 데이터의 로짓값이 3이라고 하였을 때 w에 따른 손실함수)

![시그모이드 함수의 MSE적용한 그래프](https://github.com/minchoCoin/minchoCoin.github.io/assets/62372650/5410590e-e0fe-4c9d-aba4-d09b584a46e4)

(사진3: 시그모이드 함수에 MSE를 적용한 그래프, 실제 확률이 0.5이라고 가정, 입력 데이터의 로짓값이 3이라고 하였을 때 w에 따른 손실함수)

위 두 그래프는 시그모이드 함수에 MSE를 적용한 그래프( $ y=(\frac{1}{1+e^{-wx_i}}-y_i)^2 $ )로, w에 따른 손실함수를 나타내고있다. 위 두 그래프에서 알 수 있듯, 손실함수가 클수록 파라미터가 대폭 수정될려면 손실함수가 클때 기울기가 커야하지만, 기울기가 작다. 따라서 손실함수가 클때, 학습속도가 대단히 느려지거나 기울기를 0으로 인식하여 학습을 멈출 수도 있다. 따라서 시그모이드 함수에 MSE를 적용하는 것은 하지 않는 것이 좋다.

## 시그모이드 함수의 미분
시그모이드 함수의 미분은 다음과 같이 구할 수 있다. $y=\frac{1}{g(x)}$ 일때

$$ y'=\frac{-g'(x)}{g(x)^2}$$

이므로

$$ \sigma ' (x) = \frac{-(1+e^{-x})'}{(1+e^{-x})^2} = \frac{e^{-x}}{(1+e^{-x})^2} = \frac{(1+e^{-x})-1}{(1+e^{-x})^2} = \frac{1}{1+e^{-x}} (1- \frac{1}{1+e^{-x}}) = \sigma (x) (1-\sigma (x))$$

따라서 시그모이드 함수의 미분은

$$\sigma ' (x) =  \sigma (x) (1-\sigma (x))$$

이다.

# 확률 분포와 정보 엔트로피
정보 엔트로피란 확률 분포의 무질서도나 불확실성 또는 정보 표현의 부담정도를 나타내는 것이다.

확률에 따라 무작위적으로 일어나는 사건의 결과를 효율적으로 표현해야한다고 할 때 예를 들어 사건 A와 사건 B가 동일한 확률 $frac{1}{2}$의 확률로 일어난다면 A를 0으로, B를 1로 표현하면 된다. 또한 사건 A,B,C,D가 동일한 확률 $frac{1}{4}$ 의 확률로 일어난다면 각각 00,01,10,11로 표현하면 된다.

그러나 사건 A,B,C가 동등한 확률로 일어날 때는 각각 00,01,10으로 표현하면 낭비가 발생한다. 왜냐하면 사건 3개를 표현하는 최소한의 비트 수는 $\log _2 3$ 이기 때문이다.

일반적으로 어떤 사건이 각각 $\frac{1}{a_1}, \frac{1}{a_2},...,\frac{1}{a_m}$ 확률로 일어날때 다음과 같이 정보량을 구할 수 있다.

$$ \frac{1}{a_1} \log _2 a_1 + \frac{1}{a_2} \log _2 a_2 + ... + \frac{1}{a_m} \log _2 a_m = \sum \frac{1}{a_i} \log _2 a_i$$

으로 구할 수 있다.

예를 들어 세 가지 결과가 각각 0.5, 0.25, 0.25 의 확률로 일어난다면 사건의 정보량을 위의 공식에 대입하면 1.5가 나온다. 이 정보량 1.5는 이 사건을 표현하기 위한 최소한의 정보량이며, 아래 허프만 코드를 보면서 더 논의한다.

허프만코드는 표현하려는 대상의 분포비율을 조사해 자주 나타나는 대상에는 짧은 이진 코드를, 덜 나타나는 대상에는 긴 이진 코드를 할당해 전체 비트수를 줄여주는 것이다. 세 가지 결과가 각각 0.5,0.25,0.25의 확률로 일어날때 각각 0, 10, 11을 할당해주면, 0.5의 확률로 자주 일어나는 사건은 1비트로 표현되면서 정보량이 줄어든다. 즉 1비트 * 0.5 + 2비트 + 0.25 + 2비트 * 0.25 = 1.5비트이므로, 이는 위에서 구한 정보량과 동일하다.

허프만 코드를 구하는 방법은 다음 링크를 참고하면 된다. [https://lipcoder.tistory.com/187](https://lipcoder.tistory.com/187)

이제 어떤 사건의 확률 $p_i = \frac{1}{a_i}$ 임을 이용해 다음과 같이 정보 엔트로피 H를 표현할 수 있다.

$$ H = \sum p_i \log _2 \frac{1}{p_i}$$

로그의 성질을 이용하여 다음과 같이 바꿀 수 있다. 또한 로그의 밑이 바뀌어도 대소관계는 유지되므로, 상용로그을 이용하여 다음과 같이 표현할 수 있다.

$$ H =- \sum p_i \log p_i$$

# 확률 분포의 추정과 교차 엔트로피
교차 엔트로피는 두 가지 확률 분포가 얼마나 비슷한지를 숫자 하나로 나타내는 개념이다. 앞서 정보 엔트로피에서 $-\log p_i$ 는 정보량이라고 볼 수 있다. 여기에 확률을 곱하면 