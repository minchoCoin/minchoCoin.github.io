---
title: "단층 퍼셉트론(SLP) - 선택 분류"
last_modified_at: 2023-10-06T23:53:12+09:00
categories:
    - alzza-deeplearning
tags:
    - A.I

toc: true
toc_label: "My Table of Contents"
author_profile: true

---
# 선택 분류 문제의 신경망 처리
선택 분류 문제는 몇 가지 정해진 후보 가운데 하나를 골라 답하는 문제이다.

선택 분류 신경망은 이진 판단에서처럼 각 후보 항목에 대한 로그 척도의 상대적 추천 강도, 즉 로짓값을 추정하도록 구성된다. 이때 퍼셉트론 하나가 후보 하나에 대한 로짓값을 출력하므로, 후보 수 만큼 퍼셉트론이 있어야한다.

로짓값은 상대적인 가능성을 로그를 이용해 나타낸 것이다. A항목의 로짓값이 3이고 B항목의 로짓값이 1이면, B보다 A를 답으로 추정할 확률이 $e^{3-2} = e^2$배 크다는 의미이다.

일반적으로 로짓값이 클수록 확률도 크므로 굳이 확률로 변경할 필요가 없을 것으로 생각할 수도 있지만, 로짓값만으로는 학습방법을 찾기 힘들다.

따라서 로짓값을 확률 분포로 변경하는 소프트맥스 함수와, 두 확률 분포의 차이를 보여주는 소프트맥스 교차 엔트로피를 이용하여 모델을 학습한다.

# 소프트맥스 함수
소프트맥스 함수는 로짓값 벡터를 확률 분포 벡터로 변환해주는 비선형 함수이다. 다음 글에서 살펴볼 철판 문제는 철판을 27가지의 데이터를 활용하여 7가지로 분류하여야한다. 미니배치 크기를 N이라 할때 미니배치 데이터는 [N,27]의 형태를 갖고, 퍼셉트론 7개로 구성된 단층 퍼셉트론은 [27,7] 형태의 가중치와 [7] 형태의 편향정보를 갖는다. 따라서 퍼셉트론은 텐서 연산으로 [N,7]의 형태를 만들어낸다. 이 [N,7] 의 데이터는 데이터 N개에 대해 각각 7가지 후보들의 로짓값을 나타내는 벡터들로 해석한다.

쉽게 4가지 데이터를 활용하여 3가지로 분류한다고 가정하자. 미니배치 크기를 5라고 한다면 다음과 같이 구할 수 있다(미니배치 데이터는 [5,4], 가중치는 [4,3], 편향정보 [3], 출력 값 [5,3] 단, 가중치를 앞에 두기 위해 가중치를 [3,4], 미니배치 데이터를 [4,5]로 배치한다).

$$ \begin{pmatrix} y_{11} & y_{12} & y_{13} & y_{14} & y_{15} \\
                   y_{21} & y_{22} & y_{23} & y_{24} & y_{25} \\
                    y_{31} & y_{32} & y_{33} & y_{34} & y_{35}
\end{pmatrix}
 = \begin{pmatrix} w_{11} & w_{12} & w_{13} & w_{14} \\
                    w_{21} & w_{22} & w_{23} & w_{24} \\
                    w_{31} & w_{32} & w_{33} & w_{34}
 \end{pmatrix}

 \begin{pmatrix} x_{11} & x_{12} & x_{13} & x_{14} & x_{15} \\
                x_{21} & x_{22} & x_{23} & x_{24} & x_{25} \\
                x_{31} & x_{32} & x_{33} & x_{34} & x_{35} \\
                x_{41} & x_{42} & x_{43} & x_{44} & x_{45}

 \end{pmatrix}
$$

여기서 $y_{ab}$ 에서 a는 a번째 항목, b는 b번쨰 data를 의미한다. 즉 $(y_{11},y_{21},y_{31})$ 은 첫번째 데이터(철판의 각종 데이터)를 통해 추정한 각 결과 항목(예. 철판 분류항목)의 로짓값이다. 또한 $w_{xy}$ 에서 x는 x번쨰 퍼셉트론, y는 y번쨰 입력값(4가지 입력 데이터 중 몇 번째)을 의미한다. 마지막으로, $x_{cd}$ 에서 c는 4가지 입력 데이터 중 c번째, d는 d번쨰 데이터 셋을 의미한다. 

로짓값들은 클수록 확률이 큰 것이기 때문에 굳이 확률값으로 변환하지 않고도 어떤 것을 골랐는지 확인할 수 있다. 그러나 확률을 눈으로 확인할 때와 소프트맥스 교차엔트로피 편미분에서 필요하다.

소프트맥스의 입력값은 로짓값 벡터이고 출력 값은 각 후보 항목이 정답일 확률들로 구성된 벡터이다. 확률은 합이 1이고 각 확률은 0이상 1이하의 값을 가져야하므로 소프트맥스 함수의 일반식은 다음과 같다.

$$ y_i = \frac{e^{x_i}}{e^{x_1} + ... + e^{x_n}}$$

이제 위 식을 증명해보자. 입력으로 들어온 로짓값 벡터 $x_1, ..., x_n$에서 $x_i$ 는 로짓값, 즉 상대적인 확률값이므로, 기준 확률을 $\alpha$ 라 하면 실제 확률은 $\alpha e^{x_i}$ 이다. 전체 확률 합은 1이므로 $\alpha \sum e^{x_i} = 1$ 이고, 따라서 $\alpha = \frac{1}{e^{x_1} + ... + e^{x_n}}$ 이다. 따라서 실제 확률 $y_i = \alpha e^{x_i} = \frac{e^{x_i}}{e^{x_1} + ... + e^{x_n}}$ 이다.


그러나 위 식은 계산 과정에서 오류를 일으킬 수 있기 때문에 최댓값 $x_k$를 찾아 아래와 같이 변경한다.

$$ \frac{e^{x_i-x_k}}{e^{x_1-x_k} + ... + e^{x_n - x_k}}$$

처음 소프트맥스 값을 쓰지 않는 이유는 먼저 $x_i$ 값의 제약이 없으므로 $x_i$ 가 매우 커져 $e^{x_i}$ 값이 오버플로우가 될 수 있고, $x_i$ 의 값이 모두 매우 작아지면 분모가 0으로 계산되면서 0으로 나누기 오류가 생길 수 있다.

따라서 분모와 분자를 상대적 확률값의 최대값 $e^{x_k}$로 나누는 방법을 사용한다.

이렇게 하면 $x_i - x_k \leq 0$ 이므로 $e^{x_i - x_k}$ 는 항상 0보다 크고 1보다 작거나 같다. 따라서 분모와 분자가 지나치게 커질 수 없기 때문에, 오버플로우 오류가 방지되며, $e^{x_k-x_k}$(=1) 항이 분모에 항상 있으므로, 0으로 나누기 오류도 방지된다.

# 소프트맥스 함수의 편미분
소프트맥스 함수의 입력으로 들어오는 로짓값 벡터의 모든 값 $x_i$ 가 출력으로 나오는 확률 분포의 모든 값 $y_i$에 영향을 미치므로, 편미분은 모든 $(x_i,y_i)$ 에 대해서 구해져야한다. 또한 모든 입출력 벡터의 모든 원소 쌍에 대한 편미분 $\frac{\partial y_j}{\partial x_i}$ 값으로 구성된 자코비 행렬을 구해야 한다.

각 편미분은 다음과 같이 구할 수 있다.

$$\frac{\partial y_j}{\partial x_i} = -y_i y_j (i \neq j)$$

$$\frac{\partial y_j}{\partial x_i} = y_i - y_iy_i = y_i - y_i^2 (i = j)$$

# 소프트맥스 교차 엔트로피
앞서 P에대한 Q의 교차 엔트로피는 다음과 같이 정의된다고 하였다.

$$ H(P,Q) = - \sum p_i \log q_i$$

이때 신경망이 추정한 로짓 벡터에 소프트맥스 함수를 적용한 벡터를 Q, 실제 확률 분포를 P로 하면 된다. 반대로 하면 일반적으로 실제 확률 분포는 1또는 0이므로, log 값이 0또는 $ - \infty$ 가 된다. 

또한 로짓값이 매우 작으면 log를 취할때 문제가 있으므로 아주 작은 양수값 $\epsilon$ 을 더해 다음과 같이 수정한다.

$$ H(P,Q) = - \sum p_i \log (q_i + \epsilon)$$
일반적으로 $\epsilon$ 은 아주 작은 양수값으로 정하므로 영향이 적고, $q_i$가 작다고 하더라도 $\epsilon$을 더하여도 매우 작은 확률로 계산되어 선택되지 않는다.

# 소프트맥스 교차 엔트로피의 편미분
신경망이 추정한 로짓 벡터 $x_1,...,x_n$, 정답 벡터 $p_1,...,p_n$, 로짓벡터에 소프트맥스 함수를 적용하여 나온 확률 분포 $q_1,...,q_n$ 이 있을 때 소프트맥스 교차 엔트로피의 편미분은 다음과 같이 구할 수 있다.

$$ \frac{\partial H}{\partial x_i} = q_i - p_i $$

또한 벡터로는 다음과 같이 구할 수 있다.

$$ softmax(\textbf x) - P$$

# 시그모이드 함수와 소프트맥스 함수의 관계

